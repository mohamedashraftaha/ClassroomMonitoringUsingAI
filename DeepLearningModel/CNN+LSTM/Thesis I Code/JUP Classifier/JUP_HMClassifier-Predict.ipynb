{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install opencv-python\n",
    "# !pip install tensorflow\n",
    "# !pip install matplotlib\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "# import pafy\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_HEIGHT, IMAGE_WIDTH = 64, 64\n",
    "SEQUENCE_LENGTH = 90\n",
    "CLASSES_LIST = [\"normalHead\", \"oddHead\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./LRCN_model___Date_Time_2021_12_10__13_58_06___Loss_1.2796132564544678___Accuracy_0.7021276354789734.h5\"\n",
    "##loading weights with Loss of 1.2796132564544678 and testing accuracy of 0.7021276354789734\n",
    "LRCN_model=load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path, SEQUENCE_LENGTH):\n",
    "    video = cv2.VideoCapture(path)\n",
    "\n",
    "    framesList = []\n",
    "    \n",
    "    predClass = ''\n",
    "\n",
    "    lenFrames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if(lenFrames<SEQUENCE_LENGTH):\n",
    "        return 0;\n",
    "    print(path)\n",
    "\n",
    "    framesToSkip = max(int(lenFrames/SEQUENCE_LENGTH),1)\n",
    "\n",
    "    for i in range(SEQUENCE_LENGTH):\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, i * framesToSkip)\n",
    "        success, frame = video.read() \n",
    "\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        resizedFrame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "#         plt.imshow(frame)\n",
    "#         plt.show()\n",
    "        \n",
    "        normalizedFrame = resizedFrame / 255\n",
    "        framesList.append(normalizedFrame)\n",
    "\n",
    "    predictedLabelsProbabilities = LRCN_model.predict(np.expand_dims(framesList, axis = 0))[0]\n",
    "\n",
    "    predLabel = np.argmax(predictedLabelsProbabilities)\n",
    "\n",
    "    predClass = CLASSES_LIST[predLabel]\n",
    "    # Display the predicted action along with the prediction confidence.\n",
    "    print(f'Action Predicted: {predClass}\\nConfidence: {predictedLabelsProbabilities[predLabel]}')\n",
    "        \n",
    "    video.release()\n",
    "    \n",
    "    return predClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_12_C_12.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999797344207764\n",
      "Accuracy:  1.0\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_12_C_4.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999608993530273\n",
      "Accuracy:  1.0\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_12_C_6.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9981369972229004\n",
      "Accuracy:  1.0\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_12_C_9.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999700784683228\n",
      "Accuracy:  1.0\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_2.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999866485595703\n",
      "Accuracy:  1.0\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_5.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999042749404907\n",
      "Accuracy:  1.0\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_6.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999984502792358\n",
      "Accuracy:  0.8571428571428571\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_7.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9997507929801941\n",
      "Accuracy:  0.875\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_8.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.999981164932251\n",
      "Accuracy:  0.8888888888888888\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_21_C_1.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.999983549118042\n",
      "Accuracy:  0.9\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_21_C_4.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9691628217697144\n",
      "Accuracy:  0.9090909090909091\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_21_C_5.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.7856380343437195\n",
      "Accuracy:  0.8333333333333334\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_21_C_6.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999868869781494\n",
      "Accuracy:  0.8461538461538461\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_21_C_7.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999868869781494\n",
      "Accuracy:  0.8571428571428571\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_25_C_1.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9997062087059021\n",
      "Accuracy:  0.8666666666666667\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_25_C_2.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9978944659233093\n",
      "Accuracy:  0.875\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_25_C_3.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9998146891593933\n",
      "Accuracy:  0.8823529411764706\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_25_C_4.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999980926513672\n",
      "Accuracy:  0.8333333333333334\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_25_C_5.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999979734420776\n",
      "Accuracy:  0.7894736842105263\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_37_C_1.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999927282333374\n",
      "Accuracy:  0.8\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_37_C_2.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999881982803345\n",
      "Accuracy:  0.8095238095238095\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_37_C_3.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9512788653373718\n",
      "Accuracy:  0.8181818181818182\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_37_C_4.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999734163284302\n",
      "Accuracy:  0.8260869565217391\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_37_C_6.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999982118606567\n",
      "Accuracy:  0.7916666666666666\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_8_C_1.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999122619628906\n",
      "Accuracy:  0.8\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_8_C_2.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9998362064361572\n",
      "Accuracy:  0.7692307692307693\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_8_C_5.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999969005584717\n",
      "Accuracy:  0.7407407407407407\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_8_C_6.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999961853027344\n",
      "Accuracy:  0.7142857142857143\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_12_N_2.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9819273352622986\n",
      "Accuracy:  0.6896551724137931\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_12_N_5.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.8815009593963623\n",
      "Accuracy:  0.6666666666666666\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_1_N_2.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999942779541016\n",
      "Accuracy:  0.6774193548387096\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_1_N_7.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999974966049194\n",
      "Accuracy:  0.6875\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_21_N_1.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999663829803467\n",
      "Accuracy:  0.696969696969697\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_21_N_2.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.8471510410308838\n",
      "Accuracy:  0.7058823529411765\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_21_N_4.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9965711832046509\n",
      "Accuracy:  0.6857142857142857\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_1.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999963045120239\n",
      "Accuracy:  0.6944444444444444\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_2.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999747276306152\n",
      "Accuracy:  0.7027027027027027\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_3.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9997078776359558\n",
      "Accuracy:  0.7105263157894737\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_4.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999973773956299\n",
      "Accuracy:  0.717948717948718\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_5.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999746084213257\n",
      "Accuracy:  0.725\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_1.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9997361302375793\n",
      "Accuracy:  0.7073170731707317\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_2.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9987002611160278\n",
      "Accuracy:  0.6904761904761905\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_3.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.6108267307281494\n",
      "Accuracy:  0.6744186046511628\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_4.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9990243911743164\n",
      "Accuracy:  0.6818181818181818\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_5.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999974966049194\n",
      "Accuracy:  0.6888888888888889\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_6.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999980926513672\n",
      "Accuracy:  0.6956521739130435\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_8_N_5.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.999993085861206\n",
      "Accuracy:  0.7021276595744681\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "classnameA=[\"oddHead\",\"normalHead\"]\n",
    "\n",
    "total=0\n",
    "sum=0\n",
    "\n",
    "for classname in classnameA:\n",
    "    path=f'C:\\\\Users\\\\CSE-P07-2179\\\\Documents\\\\headmovement\\\\{classname}Test'\n",
    "    videosPath=os.listdir(path)\n",
    "\n",
    "    for video in videosPath:\n",
    "        inputVideoPath=os.path.join(f'{path}\\\\{video}')\n",
    "        pred=predict(inputVideoPath, 90)\n",
    "        if(pred==classname):\n",
    "            sum+=1\n",
    "        if(pred!=0):\n",
    "            total+=1\n",
    "            acc=sum/total\n",
    "            print(\"Accuracy: \", acc)\n",
    "        \n",
    "\n",
    "print(\"DONE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frames_extraction(video_path):\n",
    "    '''\n",
    "    This function will extract the required frames from a video after resizing and normalizing them.\n",
    "    Args:\n",
    "        video_path: The path of the video in the disk, whose frames are to be extracted.\n",
    "    Returns:\n",
    "        frames_list: A list containing the resized and normalized frames of the video.\n",
    "    '''\n",
    "    # Declare a list to store video frames.\n",
    "    frames_list = []\n",
    "    # Read the Video File using the VideoCapture object.\n",
    "    video_reader = cv2.VideoCapture(video_path)\n",
    "    # Get the total number of frames in the video.\n",
    "    video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(video_frames_count)\n",
    "    # Calculate the the interval after which frames will be added to the list.\n",
    "    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    # Iterate through the Video Frames.\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "        # Set the current frame position of the video.\n",
    "        video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "        # Reading the frame from the video.\n",
    "        success, frame = video_reader.read()\n",
    "        # Check if Video frame is not successfully read then break the loop\n",
    "        if not success:\n",
    "            break\n",
    "        # Resize the Frame to fixed height and width.\n",
    "\n",
    "#         plt.imshow(frame)\n",
    "#         plt.show()\n",
    "\n",
    "        # cropped_image = frame[350:850,650:2]\n",
    "\n",
    "        # plt.imshow(cropped_image)\n",
    "        # plt.show()\n",
    "\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        # plt.imshow(resized_frame)\n",
    "        # plt.show()\n",
    "\n",
    "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1\n",
    "        normalized_frame = resized_frame / 255\n",
    "        # Append the normalized frame into the frames list\n",
    "        frames_list.append(normalized_frame)\n",
    "    # Release the VideoCapture object.\n",
    "    video_reader.release()\n",
    "    # Return the frames list.\n",
    "    return frames_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    '''\n",
    "    This function will extract the data of the selected classes and create the required dataset.\n",
    "    Returns:\n",
    "        features:          A list containing the extracted frames of the videos.\n",
    "        labels:            A list containing the indexes of the classes associated with the videos.\n",
    "        video_files_paths: A list containing the paths of the videos in the disk.\n",
    "    '''\n",
    "    # Declared Empty Lists to store the features, labels and video file path values.\n",
    "    features_train = []\n",
    "    features_test = []\n",
    "\n",
    "    labels_train = []\n",
    "    labels_test = []\n",
    "\n",
    "    video_files_paths = []\n",
    "\n",
    "    # Iterating through all the classes mentioned in the classes list\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        # Display the name of the class whose data is being extracted.\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "        # Get the list of video files present in the specific class name directory.\n",
    "        files_list = os.listdir(f'C:\\\\Users\\\\CSE-P07-2179\\\\Documents\\\\headmovement\\\\{class_name}')\n",
    "        # Iterate through all the files present in the files list.\n",
    "        for file_name in files_list:\n",
    "            # Get the complete video path.\n",
    "            video_file_path = os.path.join(f'C:\\\\Users\\\\CSE-P07-2179\\\\Documents\\\\headmovement\\\\{class_name}\\\\{file_name}')\n",
    "            print(video_file_path)\n",
    "\n",
    "            # Extract the frames of the video file.\n",
    "            frames = frames_extraction(video_file_path)\n",
    "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
    "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "                # Append the data to their repective lists.\n",
    "                features_train.append(frames)\n",
    "                labels_train.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "\n",
    "\n",
    "    for class_index, class_name in enumerate(CLASSES_LIST):\n",
    "        # Display the name of the class whose data is being extracted.\n",
    "        print(f'Extracting Data of Class: {class_name}')\n",
    "        # Get the list of video files present in the specific class name directory.\n",
    "        files_list = os.listdir(f'C:\\\\Users\\\\CSE-P07-2179\\\\Documents\\\\headmovement\\\\{class_name}Test')\n",
    "        # Iterate through all the files present in the files list.\n",
    "        for file_name in files_list:\n",
    "            # Get the complete video path.\n",
    "            video_file_path = os.path.join(f'C:\\\\Users\\\\CSE-P07-2179\\\\Documents\\\\headmovement\\\\{class_name}Test\\\\{file_name}')\n",
    "            print(video_file_path)\n",
    "            # Extract the frames of the video file.\n",
    "            frames = frames_extraction(video_file_path)\n",
    "            # Check if the extracted frames are equal to the SEQUENCE_LENGTH specified above.\n",
    "            # So ignore the vides having frames less than the SEQUENCE_LENGTH.\n",
    "            if len(frames) == SEQUENCE_LENGTH:\n",
    "                # Append the data to their repective lists.\n",
    "                features_test.append(frames)\n",
    "                labels_test.append(class_index)\n",
    "                video_files_paths.append(video_file_path)\n",
    "\n",
    "    # Converting the list to numpy arrays\n",
    "    features_train = np.asarray(features_train)\n",
    "    labels_train = np.array(labels_train)\n",
    "\n",
    "    features_test = np.asarray(features_test)\n",
    "    labels_test = np.array(labels_test)\n",
    "\n",
    "    # Return the frames, class index, and video file path.\n",
    "    return features_train,features_test, labels_train,labels_test, video_files_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Data of Class: normalHead\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_10_N_1.mp4\n",
      "64\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_10_N_2.mp4\n",
      "81\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_10_N_3.mp4\n",
      "59\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_10_N_4.mp4\n",
      "41\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_10_N_5.mp4\n",
      "160\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_1.mp4\n",
      "41\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_10.mp4\n",
      "88\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_11.mp4\n",
      "83\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_2.mp4\n",
      "84\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_3.mp4\n",
      "100\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_4.mp4\n",
      "32\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_5.mp4\n",
      "56\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_6.mp4\n",
      "71\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_7.mp4\n",
      "33\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_8.mp4\n",
      "84\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_13_N_9.mp4\n",
      "107\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_14_N_1.mp4\n",
      "38\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_14_N_10.mp4\n",
      "53\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_14_N_11.mp4\n",
      "18\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_14_N_2.mp4\n",
      "57\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_14_N_3.mp4\n",
      "112\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHead\\HM_14_N_4.mp4\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "features_train, features_test, labels_train, labels_test, video_files_paths = create_dataset()\n",
    "print(np.shape(features_train))\n",
    "print(np.shape(features_test))\n",
    "\n",
    "labels_train = to_categorical(labels_train)\n",
    "labels_test = to_categorical(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LRCN_model():\n",
    "    '''\n",
    "    This function will construct the required LRCN model.\n",
    "    Returns:\n",
    "        model: It is the required constructed LRCN model.\n",
    "    '''\n",
    "\n",
    "    # We will use a Sequential model for model construction.\n",
    "    model = Sequential()\n",
    "\n",
    "    # Define the Model Architecture.\n",
    "    ########################################################################################################################\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(16, (3, 3), padding='same', activation='relu'),\n",
    "                              input_shape=(SEQUENCE_LENGTH, IMAGE_HEIGHT, IMAGE_WIDTH, 3)))\n",
    "\n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(32, (3, 3), padding='same', activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((4, 4))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    model.add(TimeDistributed(Dropout(0.25)))\n",
    "    \n",
    "#     model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu')))\n",
    "#     model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "#     model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "    model.add(TimeDistributed(Conv2D(64, (3, 3), padding='same', activation='relu')))\n",
    "    model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
    "    # model.add(TimeDistributed(Dropout(0.25)))\n",
    "\n",
    "    model.add(TimeDistributed(Flatten()))\n",
    "\n",
    "    model.add(LSTM(64))\n",
    "\n",
    "    model.add(Dense(len(CLASSES_LIST), activation='softmax'))\n",
    "\n",
    "    ########################################################################################################################\n",
    "\n",
    "    # Display the models summary.\n",
    "    model.summary()\n",
    "\n",
    "    # Return the constructed LRCN model.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 45, 64, 64, 16)   448       \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 45, 16, 16, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 45, 16, 16, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 45, 16, 16, 32)   4640      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 45, 4, 4, 32)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 45, 4, 4, 32)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 45, 4, 4, 64)     18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 45, 2, 2, 64)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 45, 2, 2, 64)     0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 45, 2, 2, 64)     36928     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_10 (TimeDi  (None, 45, 1, 1, 64)     0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " time_distributed_11 (TimeDi  (None, 45, 64)           0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                33024     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 130       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 93,666\n",
      "Trainable params: 93,666\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "LRCN_model = create_LRCN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'features_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CSE-P0~1\\AppData\\Local\\Temp/ipykernel_14676/95926283.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# Start training the model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m LRCN_model_training_history = LRCN_model.fit(x=features_train, y=labels_train, epochs=100, batch_size=4, shuffle=True,\n\u001b[0m\u001b[0;32m      5\u001b[0m                                              validation_split=0.2)\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'features_train' is not defined"
     ]
    }
   ],
   "source": [
    "LRCN_model.compile(loss='binary_crossentropy', optimizer='Adam', metrics=[\"accuracy\"])\n",
    "\n",
    "LRCN_model_training_history = LRCN_model.fit(x=features_train, y=labels_train, epochs=100, batch_size=4, shuffle=True,\n",
    "                                             validation_split=0.2)\n",
    "\n",
    "model_evaluation_history = LRCN_model.evaluate(features_test, labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "129\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "boundedfolder = \"C:\\\\Users\\\\CSE-P07-2179\\\\Desktop\\\\Thesis\\\\headMovementDataset\\\\oddHead\\\\boundedFrames\"\n",
    "\n",
    "frames =[]\n",
    "\n",
    "# path1 =os.path.join(boundedfolder,f\"Bounded_Frame_HM_10_C_1_1.jpg\")\n",
    "# print(path1)\n",
    "# print(os.path.isfile(path1))\n",
    "for i in range(1,130):\n",
    "       #print((os.path.join(boundedfolder,f\"HM_10_C_1_1.jpg\")))\n",
    "        #frame=cv2.imread(os.path.join(boundedfolder,f\"Bounded_Frame_HM_10_C_1_{i}.jpg\") )\n",
    "        #print(frame)\n",
    "        frames.append(cv2.imread(os.path.join(boundedfolder,f\"Bounded_Frame_HM_10_C_3_{i}.jpg\") ))\n",
    "        \n",
    "        \n",
    "print(len(frames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_evaluation_loss, model_evaluation_accuracy = model_evaluation_history\n",
    "\n",
    "date_time_format = '%Y_%m_%d__%H_%M_%S'\n",
    "current_date_time_dt = dt.datetime.now()\n",
    "current_date_time_string = dt.datetime.strftime(current_date_time_dt, date_time_format)\n",
    "\n",
    "model_file_name = f'./LRCN_model___Date_Time_{current_date_time_string}___Loss_{model_evaluation_loss}___Accuracy_{model_evaluation_accuracy}.h5'\n",
    "\n",
    "LRCN_model.save(model_file_name)\n",
    "\n",
    "model_evaluation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"./LRCN_model___Date_Time_2021_12_10__13_58_06___Loss_1.2796132564544678___Accuracy_0.7021276354789734.h5\"\n",
    "\n",
    "LRCN_model=load_model(filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(path, SEQUENCE_LENGTH):\n",
    "    video = cv2.VideoCapture(path)\n",
    "\n",
    "    framesList = []\n",
    "    \n",
    "    predClass = ''\n",
    "\n",
    "    lenFrames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    if(lenFrames<SEQUENCE_LENGTH):\n",
    "        return 0;\n",
    "    print(path)\n",
    "\n",
    "    framesToSkip = max(int(lenFrames/SEQUENCE_LENGTH),1)\n",
    "\n",
    "    for i in range(SEQUENCE_LENGTH):\n",
    "        video.set(cv2.CAP_PROP_POS_FRAMES, i * framesToSkip)\n",
    "        success, frame = video.read() \n",
    "\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        resizedFrame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "#         plt.imshow(frame)\n",
    "#         plt.show()\n",
    "        \n",
    "        normalizedFrame = resizedFrame / 255\n",
    "        framesList.append(normalizedFrame)\n",
    "\n",
    "    predictedLabelsProbabilities = LRCN_model.predict(np.expand_dims(framesList, axis = 0))[0]\n",
    "\n",
    "    predLabel = np.argmax(predictedLabelsProbabilities)\n",
    "\n",
    "    predClass = CLASSES_LIST[predLabel]\n",
    "    # Display the predicted action along with the prediction confidence.\n",
    "    print(f'Action Predicted: {predClass}\\nConfidence: {predictedLabelsProbabilities[predLabel]}')\n",
    "        \n",
    "    video.release()\n",
    "    \n",
    "    return predClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_12_N_2.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9819273352622986\n",
      "Accuracy:  0.0\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_12_N_5.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.8815009593963623\n",
      "Accuracy:  0.0\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_1_N_2.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999942779541016\n",
      "Accuracy:  0.3333333333333333\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_1_N_7.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999974966049194\n",
      "Accuracy:  0.5\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_21_N_1.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999663829803467\n",
      "Accuracy:  0.6\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_21_N_2.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.8471510410308838\n",
      "Accuracy:  0.6666666666666666\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_21_N_4.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9965711832046509\n",
      "Accuracy:  0.5714285714285714\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_1.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999963045120239\n",
      "Accuracy:  0.625\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_2.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999747276306152\n",
      "Accuracy:  0.6666666666666666\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_3.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9997078776359558\n",
      "Accuracy:  0.7\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_4.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999973773956299\n",
      "Accuracy:  0.7272727272727273\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_25_N_5.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999746084213257\n",
      "Accuracy:  0.75\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_1.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9997361302375793\n",
      "Accuracy:  0.6923076923076923\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_2.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9987002611160278\n",
      "Accuracy:  0.6428571428571429\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_3.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.6108267307281494\n",
      "Accuracy:  0.6\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_4.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9990243911743164\n",
      "Accuracy:  0.625\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_5.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999974966049194\n",
      "Accuracy:  0.6470588235294118\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_37_N_6.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999980926513672\n",
      "Accuracy:  0.6666666666666666\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\normalHeadTest\\HM_8_N_5.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.999993085861206\n",
      "Accuracy:  0.6842105263157895\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_12_C_12.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999797344207764\n",
      "Accuracy:  0.7\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_12_C_4.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999608993530273\n",
      "Accuracy:  0.7142857142857143\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_12_C_6.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9981369972229004\n",
      "Accuracy:  0.7272727272727273\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_12_C_9.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999700784683228\n",
      "Accuracy:  0.7391304347826086\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_2.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999866485595703\n",
      "Accuracy:  0.75\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_5.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9999042749404907\n",
      "Accuracy:  0.76\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_6.mp4\n",
      "Action Predicted: normalHead\n",
      "Confidence: 0.9999984502792358\n",
      "Accuracy:  0.7307692307692307\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_7.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.9997507929801941\n",
      "Accuracy:  0.7407407407407407\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_1_C_8.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.999981164932251\n",
      "Accuracy:  0.75\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_21_C_1.mp4\n",
      "Action Predicted: oddHead\n",
      "Confidence: 0.999983549118042\n",
      "Accuracy:  0.7586206896551724\n",
      "C:\\Users\\CSE-P07-2179\\Documents\\headmovement\\oddHeadTest\\HM_21_C_2.mp4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 90, 64, 64, 3), found shape=(None, 89, 64, 64, 3)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\CSE-P0~1\\AppData\\Local\\Temp/ipykernel_14620/1471292406.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mvideo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvideosPath\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0minputVideoPath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{path}\\\\{video}'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m         \u001b[0mpred\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputVideoPath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m90\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mclassname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0msum\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\CSE-P0~1\\AppData\\Local\\Temp/ipykernel_14620/616119840.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(path, SEQUENCE_LENGTH)\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mframesList\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalizedFrame\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mpredictedLabelsProbabilities\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLRCN_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mframesList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mpredLabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredictedLabelsProbabilities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1621, in predict_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1611, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1604, in run_step  **\n        outputs = model.predict_step(data)\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1572, in predict_step\n        return self(x, training=False)\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\CSE-P07-2179\\anaconda3\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"sequential_1\" is incompatible with the layer: expected shape=(None, 90, 64, 64, 3), found shape=(None, 89, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "classnameA=[\"normalHead\",\"oddHead\"]\n",
    "\n",
    "total=0\n",
    "sum=0\n",
    "\n",
    "for classname in classnameA:\n",
    "    path=f'C:\\\\Users\\\\CSE-P07-2179\\\\Documents\\\\headmovement\\\\{classname}Test'\n",
    "    videosPath=os.listdir(path)\n",
    "\n",
    "    for video in videosPath:\n",
    "        inputVideoPath=os.path.join(f'{path}\\\\{video}')\n",
    "        pred=predict(inputVideoPath, 90)\n",
    "        if(pred==classname):\n",
    "            sum+=1\n",
    "        if(pred!=0):\n",
    "            total+=1\n",
    "            acc=sum/total\n",
    "            print(\"Accuracy: \", acc)\n",
    "        \n",
    "\n",
    "print(\"DONE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classnameA=[\"normalHead\",\"oddHead\"]\n",
    "\n",
    "total=0\n",
    "sum=0\n",
    "\n",
    "for classname in classnameA:\n",
    "    path=f'C:\\\\Users\\\\CSE-P07-2179\\\\Documents\\\\headmovement\\\\{classname}Test'\n",
    "    videosPath=os.listdir(path)\n",
    "\n",
    "    for video in videosPath:\n",
    "        inputVideoPath=os.path.join(f'{path}\\\\{video}')\n",
    "\n",
    "        pred=predict(inputVideoPath, 90)\n",
    "        if(pred==classname):\n",
    "            sum+=1\n",
    "        if(pred!=0):\n",
    "            total+=1\n",
    "            acc=sum/total\n",
    "            print(acc)\n",
    "        \n",
    "\n",
    "# boundedfolder\n",
    "print(\"DONE\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_action2(video_reader, SEQUENCE_LENGTH):\n",
    "\n",
    "    # Initialize the VideoCapture object to read from the video file.\n",
    "    # video_reader = cv2.VideoCapture(video_reader)\n",
    "    # Get the width and height of the video.\n",
    "    # original_video_width = int(video_reader.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    # original_video_height = int(video_reader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    print(\"here\")\n",
    "    frames_list = []\n",
    "\n",
    "    predicted_class_name = ''\n",
    "\n",
    "    # Get the number of frames in the video.\n",
    "    # video_frames_count = int(video_reader.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    video_frames_count = len(video_reader)\n",
    "    if (video_frames_count < SEQUENCE_LENGTH - 1):\n",
    "        print(video_frames_count)\n",
    "        return 0\n",
    "    # print(video_file_path)\n",
    "\n",
    "    # Calculate the interval after which frames will be added to the list.\n",
    "    skip_frames_window = max(int(video_frames_count / SEQUENCE_LENGTH), 1)\n",
    "\n",
    "    # Iterating the number of times equal to the fixed length of sequence.\n",
    "    for frame_counter in range(SEQUENCE_LENGTH):\n",
    "\n",
    "        # Set the current frame position of the video.\n",
    "        # video_reader.set(cv2.CAP_PROP_POS_FRAMES, frame_counter * skip_frames_window)\n",
    "        frame=video_reader[frame_counter+skip_frames_window]\n",
    "\n",
    "\n",
    "\n",
    "        # Read a frame.\n",
    "        # success, frame = video_reader.read()\n",
    "\n",
    "        # Check if frame is not read properly then break the loop.\n",
    "        # if not success:\n",
    "        #     break\n",
    "\n",
    "        # Resize the Frame to fixed Dimensions.\n",
    "        resized_frame = cv2.resize(frame, (IMAGE_HEIGHT, IMAGE_WIDTH))\n",
    "        # plt.imshow(frame)\n",
    "        # plt.show()\n",
    "\n",
    "        # Normalize the resized frame by dividing it with 255 so that each pixel value then lies between 0 and 1.\n",
    "        normalized_frame = resized_frame / 255\n",
    "\n",
    "        # Appending the pre-processed frame into the frames list\n",
    "        frames_list.append(normalized_frame)\n",
    "\n",
    "    # Passing the  pre-processed frames to the model and get the predicted probabilities.\n",
    "    predicted_labels_probabilities = LRCN_model.predict(np.expand_dims(frames_list, axis=0))[0]\n",
    "\n",
    "    # Get the index of class with highest probability.\n",
    "    predicted_label = np.argmax(predicted_labels_probabilities)\n",
    "\n",
    "    # Get the class name using the retrieved index.\n",
    "    predicted_class_name = CLASSES_LIST[predicted_label]\n",
    "\n",
    "    # Display the predicted action along with the prediction confidence.\n",
    "    print(f'Action Predicted: {predicted_class_name}\\nConfidence: {predicted_labels_probabilities[predicted_label]}')\n",
    "\n",
    "    # Release the VideoCapture object.\n",
    "\n",
    "    return predicted_class_name\n",
    "\n",
    "# predict_single_action(f'C:\\\\Users\\\\CSE-P07-2179\\\\Documents\\\\headmovement\\\\oddHeadTest\\\\HM_12_C_12.mp4', 90)   \n",
    "x= predict_single_action2(frames, 90)   \n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
